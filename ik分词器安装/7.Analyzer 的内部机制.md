Analyzer 一般由三部分构成，character filters、tokenizers、token filters。


Elasticsearch 有10种分词器（Tokenizer）、31种 token filter，3种 character filter，一大堆配置项。此外，还有还可以安装 plugin 扩展功能。这些都是搭建 analyzer 的原材料。




Analyzer 的组成要素
Analyzer 的内部就是一条流水线

Step 1 字符过滤（Character filter）
Step 2 分词 （Tokenization）
Step 3 Token 过滤（Token filtering）




PUT /my-index/_settings
{
  "index": {
    "analysis": {
      "analyzer": {
        "customHTMLSnowball": {
         "type": "custom",
          "char_filter": [
            "html_strip"
          ],
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "stop",
            "snowball"
          ]
        }}}}}
以上自定义的 Analyzer名为 customHTMLSnowball， 代表的含义：

移除 html 标签 （html_strip character filter），比如 <p> <a> <div> 。

分词，去除标点符号（standard tokenizer）

把大写的单词转为小写（lowercase token filter）

过滤停用词（stop token filter），比如 「the」 「they」 「i」 「a」 「an」 「and」。

提取词干（snowball token filter，snowball 雪球算法是提取英文词干最常用的一种算法。）

cats -> cat

catty -> cat

stemmer -> stem

stemming -> stem

stemmed -> stem

https://mednoter.com/all-about-analyzer-part-one.html
https://blog.csdn.net/u013200380/article/details/106887305



首先要区分 分析器(analyzer)和分词器(tokenizer) 的区别，在ES中，分析器不等于分词器，分词器只是分析器的一部分。
可以通过下面的公式来进行区分：
analyzer = [char_filter] + tokenizer + [token filter]
一个ES分析器包含下面三部分：


char filter： 对输入的文本字符进行第一步处理，如去除html标签（html_strip），将表情字符转换成英文单词（mapping）等。ES内置的字符处理器可参考 char filter reference


tokenizer： 对文本进行分词操作，如按照空格分词（whitespace），标准分词器（standard）等，切分好的单元在ES（Lucene？）中被定义为 token。ES内置的分词器可参考 tokenizer reference


filter （token filter）： 对一个token集合的元素做过滤和转换(修改)，删除等操作。例如可以将whitespace tokenizer切割的单元转换为词干形式(driven-->drive)，统一转换为小写形式(lowercase)，过滤掉一些停用词（stop）等。ES有丰富的内置的token filter，详细可以参考这里 token filter reference 。 token经过 filter处理之后的结果被定义为：term。

https://juejin.cn/post/6939396159983222815